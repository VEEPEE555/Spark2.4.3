{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " big.csv\t\t       'Praveen Vasudev_07_07_2019.docx'\r\n",
      " code-master\t\t       'Praveen Vasudev_2017.docx'\r\n",
      " Codility-Python-master        'Praveen Vasudev_2019.docx'\r\n",
      " credentials.csv\t       'Praveen Vasudev_2019.pdf'\r\n",
      " Crimes_-_2001_to_present.csv   sarakey.pem\r\n",
      " data-master\t\t        Spark-practice-master\r\n",
      " iris.csv\r\n"
     ]
    }
   ],
   "source": [
    "#Exercise 01 - Get monthly crime count by type\n",
    "#data set location \" https://data.cityofchicago.org/Public-Safety/Crimes-2001-to-present/ijzp-q8t2 \"\n",
    "!ls /home/veepee555/Downloads/\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import *\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".appName('exerciseOne4Spark') \\\n",
    ".getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimesDF = spark \\\n",
    ".read \\\n",
    ".format(\"csv\") \\\n",
    ".option(\"header\",\"true\") \\\n",
    ".option(\"mode\",\"DROPMALFORMED\") \\\n",
    ".option(\"inferSchema\",\"true\") \\\n",
    ".load('/home/veepee555/Downloads/Crimes_-_2001_to_present.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(crimesDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Case Number: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Block: string (nullable = true)\n",
      " |-- IUCR: string (nullable = true)\n",
      " |-- Primary Type: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Location Description: string (nullable = true)\n",
      " |-- Arrest: boolean (nullable = true)\n",
      " |-- Domestic: boolean (nullable = true)\n",
      " |-- Beat: integer (nullable = true)\n",
      " |-- District: integer (nullable = true)\n",
      " |-- Ward: integer (nullable = true)\n",
      " |-- Community Area: integer (nullable = true)\n",
      " |-- FBI Code: string (nullable = true)\n",
      " |-- X Coordinate: integer (nullable = true)\n",
      " |-- Y Coordinate: integer (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Updated On: string (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Historical Wards 2003-2015: integer (nullable = true)\n",
      " |-- Zip Codes: integer (nullable = true)\n",
      " |-- Community Areas: integer (nullable = true)\n",
      " |-- Census Tracts: integer (nullable = true)\n",
      " |-- Wards: integer (nullable = true)\n",
      " |-- Boundaries - ZIP Codes: integer (nullable = true)\n",
      " |-- Police Districts: integer (nullable = true)\n",
      " |-- Police Beats: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crimesDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method createOrReplaceTempView in module pyspark.sql.dataframe:\n",
      "\n",
      "createOrReplaceTempView(name) method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Creates or replaces a local temporary view with this DataFrame.\n",
      "    \n",
      "    The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
      "    that was used to create this :class:`DataFrame`.\n",
      "    \n",
      "    >>> df.createOrReplaceTempView(\"people\")\n",
      "    >>> df2 = df.filter(df.age > 3)\n",
      "    >>> df2.createOrReplaceTempView(\"people\")\n",
      "    >>> df3 = spark.sql(\"select * from people\")\n",
      "    >>> sorted(df3.collect()) == sorted(df2.collect())\n",
      "    True\n",
      "    >>> spark.catalog.dropTempView(\"people\")\n",
      "    \n",
      "    .. versionadded:: 2.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(crimesDF.createOrReplaceTempView)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+\n",
      "|            col_name|data_type|comment|\n",
      "+--------------------+---------+-------+\n",
      "|                  ID|      int|   null|\n",
      "|         Case Number|   string|   null|\n",
      "|                Date|   string|   null|\n",
      "|               Block|   string|   null|\n",
      "|                IUCR|   string|   null|\n",
      "|        Primary Type|   string|   null|\n",
      "|         Description|   string|   null|\n",
      "|Location Description|   string|   null|\n",
      "|              Arrest|  boolean|   null|\n",
      "|            Domestic|  boolean|   null|\n",
      "|                Beat|      int|   null|\n",
      "|            District|      int|   null|\n",
      "|                Ward|      int|   null|\n",
      "|      Community Area|      int|   null|\n",
      "|            FBI Code|   string|   null|\n",
      "|        X Coordinate|      int|   null|\n",
      "|        Y Coordinate|      int|   null|\n",
      "|                Year|      int|   null|\n",
      "|          Updated On|   string|   null|\n",
      "|            Latitude|   double|   null|\n",
      "+--------------------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+-------------+-----------+\n",
      "|database|    tableName|isTemporary|\n",
      "+--------+-------------+-----------+\n",
      "|        |chicagocrimes|       true|\n",
      "+--------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crimesDF.createOrReplaceTempView('ChicagoCrimes')\n",
    "spark.sql(\"describe ChicagoCrimes\").show()\n",
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------+------------+--------+\n",
      "|concat(substring(Date, 7, 4), substring(Date, 7, 4))|Primary Type|count(1)|\n",
      "+----------------------------------------------------+------------+--------+\n",
      "|                                            20012001|Primary Type|  485755|\n",
      "|                                            20022002|Primary Type|  486757|\n",
      "|                                            20032003|Primary Type|  475947|\n",
      "|                                            20042004|Primary Type|  469385|\n",
      "|                                            20052005|Primary Type|  453719|\n",
      "|                                            20062006|Primary Type|  448116|\n",
      "|                                            20072007|Primary Type|  437019|\n",
      "|                                            20082008|Primary Type|  427068|\n",
      "|                                            20092009|Primary Type|  392702|\n",
      "|                                            20102010|Primary Type|  370331|\n",
      "|                                            20112011|Primary Type|  351806|\n",
      "|                                            20122012|Primary Type|  336013|\n",
      "|                                            20132013|Primary Type|  307143|\n",
      "|                                            20142014|Primary Type|  275356|\n",
      "|                                            20152015|Primary Type|  264188|\n",
      "|                                            20162016|Primary Type|  269102|\n",
      "|                                            20172017|Primary Type|  268252|\n",
      "|                                            20182018|Primary Type|  267173|\n",
      "|                                            20192019|Primary Type|  133318|\n",
      "+----------------------------------------------------+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Get monthly count of primary crime type, sorted by month in ascending and number of crimes per type in descending order\n",
    "\n",
    "spark.sql(\"select  \\\n",
    "concat(substr(Date,7,4),substr(Date,7,4)), \\\n",
    "'Primary Type', count(1)   \\\n",
    "from ChicagoCrimes  \\\n",
    "group by concat(substr(Date,7,4),substr(Date,7,4)) ,'Primary Type' \\\n",
    "order by concat(substr(Date,7,4),substr(Date,7,4)) asc,'Primary Type' desc , count(1) desc  \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultDF=spark.sql(\"select  \\\n",
    "concat(substr(Date,7,4),substr(Date,7,4)), \\\n",
    "'Primary Type', count(1)   \\\n",
    "from ChicagoCrimes  \\\n",
    "group by concat(substr(Date,7,4),substr(Date,7,4)) ,'Primary Type' \\\n",
    "order by concat(substr(Date,7,4),substr(Date,7,4)) asc,'Primary Type' desc , count(1) desc  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on DataFrameWriter in module pyspark.sql.readwriter object:\n",
      "\n",
      "class DataFrameWriter(OptionUtils)\n",
      " |  DataFrameWriter(df)\n",
      " |  \n",
      " |  Interface used to write a :class:`DataFrame` to external storage systems\n",
      " |  (e.g. file systems, key-value stores, etc). Use :func:`DataFrame.write`\n",
      " |  to access this.\n",
      " |  \n",
      " |  .. versionadded:: 1.4\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DataFrameWriter\n",
      " |      OptionUtils\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, df)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  bucketBy(self, numBuckets, col, *cols)\n",
      " |      Buckets the output by the given columns.If specified,\n",
      " |      the output is laid out on the file system similar to Hive's bucketing scheme.\n",
      " |      \n",
      " |      :param numBuckets: the number of buckets to save\n",
      " |      :param col: a name of a column, or a list of names.\n",
      " |      :param cols: additional names (optional). If `col` is a list it should be empty.\n",
      " |      \n",
      " |      .. note:: Applicable for file-based data sources in combination with\n",
      " |                :py:meth:`DataFrameWriter.saveAsTable`.\n",
      " |      \n",
      " |      >>> (df.write.format('parquet')  # doctest: +SKIP\n",
      " |      ...     .bucketBy(100, 'year', 'month')\n",
      " |      ...     .mode(\"overwrite\")\n",
      " |      ...     .saveAsTable('bucketed_table'))\n",
      " |      \n",
      " |      .. versionadded:: 2.3\n",
      " |  \n",
      " |  csv(self, path, mode=None, compression=None, sep=None, quote=None, escape=None, header=None, nullValue=None, escapeQuotes=None, quoteAll=None, dateFormat=None, timestampFormat=None, ignoreLeadingWhiteSpace=None, ignoreTrailingWhiteSpace=None, charToEscapeQuoteEscaping=None, encoding=None, emptyValue=None)\n",
      " |      Saves the content of the :class:`DataFrame` in CSV format at the specified path.\n",
      " |      \n",
      " |      :param path: the path in any Hadoop supported file system\n",
      " |      :param mode: specifies the behavior of the save operation when data already exists.\n",
      " |      \n",
      " |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      " |          * ``overwrite``: Overwrite existing data.\n",
      " |          * ``ignore``: Silently ignore this operation if data already exists.\n",
      " |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already \\\n",
      " |              exists.\n",
      " |      \n",
      " |      :param compression: compression codec to use when saving to file. This can be one of the\n",
      " |                          known case-insensitive shorten names (none, bzip2, gzip, lz4,\n",
      " |                          snappy and deflate).\n",
      " |      :param sep: sets a single character as a separator for each field and value. If None is\n",
      " |                  set, it uses the default value, ``,``.\n",
      " |      :param quote: sets a single character used for escaping quoted values where the\n",
      " |                    separator can be part of the value. If None is set, it uses the default\n",
      " |                    value, ``\"``. If an empty string is set, it uses ``u0000`` (null character).\n",
      " |      :param escape: sets a single character used for escaping quotes inside an already\n",
      " |                     quoted value. If None is set, it uses the default value, ``\\``\n",
      " |      :param escapeQuotes: a flag indicating whether values containing quotes should always\n",
      " |                           be enclosed in quotes. If None is set, it uses the default value\n",
      " |                           ``true``, escaping all values containing a quote character.\n",
      " |      :param quoteAll: a flag indicating whether all values should always be enclosed in\n",
      " |                        quotes. If None is set, it uses the default value ``false``,\n",
      " |                        only escaping values containing a quote character.\n",
      " |      :param header: writes the names of columns as the first line. If None is set, it uses\n",
      " |                     the default value, ``false``.\n",
      " |      :param nullValue: sets the string representation of a null value. If None is set, it uses\n",
      " |                        the default value, empty string.\n",
      " |      :param dateFormat: sets the string that indicates a date format. Custom date formats\n",
      " |                         follow the formats at ``java.text.SimpleDateFormat``. This\n",
      " |                         applies to date type. If None is set, it uses the\n",
      " |                         default value, ``yyyy-MM-dd``.\n",
      " |      :param timestampFormat: sets the string that indicates a timestamp format. Custom date\n",
      " |                              formats follow the formats at ``java.text.SimpleDateFormat``.\n",
      " |                              This applies to timestamp type. If None is set, it uses the\n",
      " |                              default value, ``yyyy-MM-dd'T'HH:mm:ss.SSSXXX``.\n",
      " |      :param ignoreLeadingWhiteSpace: a flag indicating whether or not leading whitespaces from\n",
      " |                                      values being written should be skipped. If None is set, it\n",
      " |                                      uses the default value, ``true``.\n",
      " |      :param ignoreTrailingWhiteSpace: a flag indicating whether or not trailing whitespaces from\n",
      " |                                       values being written should be skipped. If None is set, it\n",
      " |                                       uses the default value, ``true``.\n",
      " |      :param charToEscapeQuoteEscaping: sets a single character used for escaping the escape for\n",
      " |                                        the quote character. If None is set, the default value is\n",
      " |                                        escape character when escape and quote characters are\n",
      " |                                        different, ``\\0`` otherwise..\n",
      " |      :param encoding: sets the encoding (charset) of saved csv files. If None is set,\n",
      " |                       the default UTF-8 charset will be used.\n",
      " |      :param emptyValue: sets the string representation of an empty value. If None is set, it uses\n",
      " |                         the default value, ``\"\"``.\n",
      " |      \n",
      " |      >>> df.write.csv(os.path.join(tempfile.mkdtemp(), 'data'))\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  format(self, source)\n",
      " |      Specifies the underlying output data source.\n",
      " |      \n",
      " |      :param source: string, name of the data source, e.g. 'json', 'parquet'.\n",
      " |      \n",
      " |      >>> df.write.format('json').save(os.path.join(tempfile.mkdtemp(), 'data'))\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  insertInto(self, tableName, overwrite=False)\n",
      " |      Inserts the content of the :class:`DataFrame` to the specified table.\n",
      " |      \n",
      " |      It requires that the schema of the class:`DataFrame` is the same as the\n",
      " |      schema of the table.\n",
      " |      \n",
      " |      Optionally overwriting any existing data.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  jdbc(self, url, table, mode=None, properties=None)\n",
      " |      Saves the content of the :class:`DataFrame` to an external database table via JDBC.\n",
      " |      \n",
      " |      .. note:: Don't create too many partitions in parallel on a large cluster;\n",
      " |          otherwise Spark might crash your external database systems.\n",
      " |      \n",
      " |      :param url: a JDBC URL of the form ``jdbc:subprotocol:subname``\n",
      " |      :param table: Name of the table in the external database.\n",
      " |      :param mode: specifies the behavior of the save operation when data already exists.\n",
      " |      \n",
      " |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      " |          * ``overwrite``: Overwrite existing data.\n",
      " |          * ``ignore``: Silently ignore this operation if data already exists.\n",
      " |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n",
      " |      :param properties: a dictionary of JDBC database connection arguments. Normally at\n",
      " |                         least properties \"user\" and \"password\" with their corresponding values.\n",
      " |                         For example { 'user' : 'SYSTEM', 'password' : 'mypassword' }\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  json(self, path, mode=None, compression=None, dateFormat=None, timestampFormat=None, lineSep=None, encoding=None)\n",
      " |      Saves the content of the :class:`DataFrame` in JSON format\n",
      " |      (`JSON Lines text format or newline-delimited JSON <http://jsonlines.org/>`_) at the\n",
      " |      specified path.\n",
      " |      \n",
      " |      :param path: the path in any Hadoop supported file system\n",
      " |      :param mode: specifies the behavior of the save operation when data already exists.\n",
      " |      \n",
      " |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      " |          * ``overwrite``: Overwrite existing data.\n",
      " |          * ``ignore``: Silently ignore this operation if data already exists.\n",
      " |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n",
      " |      :param compression: compression codec to use when saving to file. This can be one of the\n",
      " |                          known case-insensitive shorten names (none, bzip2, gzip, lz4,\n",
      " |                          snappy and deflate).\n",
      " |      :param dateFormat: sets the string that indicates a date format. Custom date formats\n",
      " |                         follow the formats at ``java.text.SimpleDateFormat``. This\n",
      " |                         applies to date type. If None is set, it uses the\n",
      " |                         default value, ``yyyy-MM-dd``.\n",
      " |      :param timestampFormat: sets the string that indicates a timestamp format. Custom date\n",
      " |                              formats follow the formats at ``java.text.SimpleDateFormat``.\n",
      " |                              This applies to timestamp type. If None is set, it uses the\n",
      " |                              default value, ``yyyy-MM-dd'T'HH:mm:ss.SSSXXX``.\n",
      " |      :param encoding: specifies encoding (charset) of saved json files. If None is set,\n",
      " |                      the default UTF-8 charset will be used.\n",
      " |      :param lineSep: defines the line separator that should be used for writing. If None is\n",
      " |                      set, it uses the default value, ``\\n``.\n",
      " |      \n",
      " |      >>> df.write.json(os.path.join(tempfile.mkdtemp(), 'data'))\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  mode(self, saveMode)\n",
      " |      Specifies the behavior when data or table already exists.\n",
      " |      \n",
      " |      Options include:\n",
      " |      \n",
      " |      * `append`: Append contents of this :class:`DataFrame` to existing data.\n",
      " |      * `overwrite`: Overwrite existing data.\n",
      " |      * `error` or `errorifexists`: Throw an exception if data already exists.\n",
      " |      * `ignore`: Silently ignore this operation if data already exists.\n",
      " |      \n",
      " |      >>> df.write.mode('append').parquet(os.path.join(tempfile.mkdtemp(), 'data'))\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  option(self, key, value)\n",
      " |      Adds an output option for the underlying data source.\n",
      " |      \n",
      " |      You can set the following option(s) for writing files:\n",
      " |          * ``timeZone``: sets the string that indicates a timezone to be used to format\n",
      " |              timestamps in the JSON/CSV datasources or partition values.\n",
      " |              If it isn't set, it uses the default value, session local timezone.\n",
      " |      \n",
      " |      .. versionadded:: 1.5\n",
      " |  \n",
      " |  options(self, **options)\n",
      " |      Adds output options for the underlying data source.\n",
      " |      \n",
      " |      You can set the following option(s) for writing files:\n",
      " |          * ``timeZone``: sets the string that indicates a timezone to be used to format\n",
      " |              timestamps in the JSON/CSV datasources or partition values.\n",
      " |              If it isn't set, it uses the default value, session local timezone.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  orc(self, path, mode=None, partitionBy=None, compression=None)\n",
      " |      Saves the content of the :class:`DataFrame` in ORC format at the specified path.\n",
      " |      \n",
      " |      :param path: the path in any Hadoop supported file system\n",
      " |      :param mode: specifies the behavior of the save operation when data already exists.\n",
      " |      \n",
      " |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      " |          * ``overwrite``: Overwrite existing data.\n",
      " |          * ``ignore``: Silently ignore this operation if data already exists.\n",
      " |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n",
      " |      :param partitionBy: names of partitioning columns\n",
      " |      :param compression: compression codec to use when saving to file. This can be one of the\n",
      " |                          known case-insensitive shorten names (none, snappy, zlib, and lzo).\n",
      " |                          This will override ``orc.compress`` and\n",
      " |                          ``spark.sql.orc.compression.codec``. If None is set, it uses the value\n",
      " |                          specified in ``spark.sql.orc.compression.codec``.\n",
      " |      \n",
      " |      >>> orc_df = spark.read.orc('python/test_support/sql/orc_partitioned')\n",
      " |      >>> orc_df.write.orc(os.path.join(tempfile.mkdtemp(), 'data'))\n",
      " |      \n",
      " |      .. versionadded:: 1.5\n",
      " |  \n",
      " |  parquet(self, path, mode=None, partitionBy=None, compression=None)\n",
      " |      Saves the content of the :class:`DataFrame` in Parquet format at the specified path.\n",
      " |      \n",
      " |      :param path: the path in any Hadoop supported file system\n",
      " |      :param mode: specifies the behavior of the save operation when data already exists.\n",
      " |      \n",
      " |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      " |          * ``overwrite``: Overwrite existing data.\n",
      " |          * ``ignore``: Silently ignore this operation if data already exists.\n",
      " |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n",
      " |      :param partitionBy: names of partitioning columns\n",
      " |      :param compression: compression codec to use when saving to file. This can be one of the\n",
      " |                          known case-insensitive shorten names (none, uncompressed, snappy, gzip,\n",
      " |                          lzo, brotli, lz4, and zstd). This will override\n",
      " |                          ``spark.sql.parquet.compression.codec``. If None is set, it uses the\n",
      " |                          value specified in ``spark.sql.parquet.compression.codec``.\n",
      " |      \n",
      " |      >>> df.write.parquet(os.path.join(tempfile.mkdtemp(), 'data'))\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  partitionBy(self, *cols)\n",
      " |      Partitions the output by the given columns on the file system.\n",
      " |      \n",
      " |      If specified, the output is laid out on the file system similar\n",
      " |      to Hive's partitioning scheme.\n",
      " |      \n",
      " |      :param cols: name of columns\n",
      " |      \n",
      " |      >>> df.write.partitionBy('year', 'month').parquet(os.path.join(tempfile.mkdtemp(), 'data'))\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  save(self, path=None, format=None, mode=None, partitionBy=None, **options)\n",
      " |      Saves the contents of the :class:`DataFrame` to a data source.\n",
      " |      \n",
      " |      The data source is specified by the ``format`` and a set of ``options``.\n",
      " |      If ``format`` is not specified, the default data source configured by\n",
      " |      ``spark.sql.sources.default`` will be used.\n",
      " |      \n",
      " |      :param path: the path in a Hadoop supported file system\n",
      " |      :param format: the format used to save\n",
      " |      :param mode: specifies the behavior of the save operation when data already exists.\n",
      " |      \n",
      " |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      " |          * ``overwrite``: Overwrite existing data.\n",
      " |          * ``ignore``: Silently ignore this operation if data already exists.\n",
      " |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n",
      " |      :param partitionBy: names of partitioning columns\n",
      " |      :param options: all other string options\n",
      " |      \n",
      " |      >>> df.write.mode('append').parquet(os.path.join(tempfile.mkdtemp(), 'data'))\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  saveAsTable(self, name, format=None, mode=None, partitionBy=None, **options)\n",
      " |      Saves the content of the :class:`DataFrame` as the specified table.\n",
      " |      \n",
      " |      In the case the table already exists, behavior of this function depends on the\n",
      " |      save mode, specified by the `mode` function (default to throwing an exception).\n",
      " |      When `mode` is `Overwrite`, the schema of the :class:`DataFrame` does not need to be\n",
      " |      the same as that of the existing table.\n",
      " |      \n",
      " |      * `append`: Append contents of this :class:`DataFrame` to existing data.\n",
      " |      * `overwrite`: Overwrite existing data.\n",
      " |      * `error` or `errorifexists`: Throw an exception if data already exists.\n",
      " |      * `ignore`: Silently ignore this operation if data already exists.\n",
      " |      \n",
      " |      :param name: the table name\n",
      " |      :param format: the format used to save\n",
      " |      :param mode: one of `append`, `overwrite`, `error`, `errorifexists`, `ignore`                      (default: error)\n",
      " |      :param partitionBy: names of partitioning columns\n",
      " |      :param options: all other string options\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  sortBy(self, col, *cols)\n",
      " |      Sorts the output in each bucket by the given columns on the file system.\n",
      " |      \n",
      " |      :param col: a name of a column, or a list of names.\n",
      " |      :param cols: additional names (optional). If `col` is a list it should be empty.\n",
      " |      \n",
      " |      >>> (df.write.format('parquet')  # doctest: +SKIP\n",
      " |      ...     .bucketBy(100, 'year', 'month')\n",
      " |      ...     .sortBy('day')\n",
      " |      ...     .mode(\"overwrite\")\n",
      " |      ...     .saveAsTable('sorted_bucketed_table'))\n",
      " |      \n",
      " |      .. versionadded:: 2.3\n",
      " |  \n",
      " |  text(self, path, compression=None, lineSep=None)\n",
      " |      Saves the content of the DataFrame in a text file at the specified path.\n",
      " |      \n",
      " |      :param path: the path in any Hadoop supported file system\n",
      " |      :param compression: compression codec to use when saving to file. This can be one of the\n",
      " |                          known case-insensitive shorten names (none, bzip2, gzip, lz4,\n",
      " |                          snappy and deflate).\n",
      " |      :param lineSep: defines the line separator that should be used for writing. If None is\n",
      " |                      set, it uses the default value, ``\\n``.\n",
      " |      \n",
      " |      The DataFrame must have only one column that is of string type.\n",
      " |      Each row becomes a new line in the output file.\n",
      " |      \n",
      " |      .. versionadded:: 1.6\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from OptionUtils:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(resultDF.write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultDF \\\n",
    ".write \\\n",
    ".format(\"csv\") \\\n",
    ".option(\"compression\", \"org.apache.hadoop.io.compress.GzipCodec\") \\\n",
    ".option(\"sep\", \"\\t\") \\\n",
    ".save(\"/home/veepee555/Downloads/result.tsv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
